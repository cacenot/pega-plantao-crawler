"""Crawler para buscar mÃ©dicos no portal do CFM usando Playwright."""

import asyncio
import json
import math
import time
from pathlib import Path

import asyncpg
from playwright.async_api import Page, Response
from pydantic import TypeAdapter

from .db import captcha as captcha_db
from .db.doctors import upsert_doctors_batch
from .db.executions import (
    check_execution_complete,
    check_state_complete,
    complete_execution_state,
    fail_execution_state,
    get_pending_pages,
    get_pending_states,
    initialize_pages,
    mark_page_failed,
    mark_pages_fetched_batch,
    start_execution,
    start_execution_state,
    pause_execution,
    update_execution_state,
)
from .models import Medico, MedicoFotoRaw, MedicoRaw, translate_keys_to_en
from ..shared.specialty_parser import parse_specialties
from ..shared.text_utils import title_case_br

# UFs do Brasil
UFS = [
    "AC",
    "AL",
    "AM",
    "AP",
    "BA",
    "CE",
    "DF",
    "ES",
    "GO",
    "MA",
    "MG",
    "MS",
    "MT",
    "PA",
    "PB",
    "PE",
    "PI",
    "PR",
    "RJ",
    "RN",
    "RO",
    "RR",
    "RS",
    "SC",
    "SE",
    "SP",
    "TO",
]

CFM_BASE_URL = "https://portal.cfm.org.br"
CFM_BUSCA_URL = f"{CFM_BASE_URL}/api_rest_php/api/v2/medicos/buscar_medicos"
CFM_FOTO_URL = f"{CFM_BASE_URL}/api_rest_php/api/v2/medicos/buscar_foto/"
CFM_PAGE_URL = f"{CFM_BASE_URL}/busca-medicos"


def _build_search_payload(
    captcha_token: str,
    uf: str,
    municipio: str = "",
    page: int = 1,
    page_size: int = 100,
) -> list[dict]:
    """Monta o payload de busca de mÃ©dicos."""
    return [
        {
            "useCaptchav2": True,
            "captcha": captcha_token,
            "medico": {
                "nome": "",
                "ufMedico": uf,
                "crmMedico": "",
                "municipioMedico": municipio,
                "tipoInscricaoMedico": "",
                "situacaoMedico": "",
                "detalheSituacaoMedico": "",
                "especialidadeMedico": "",
                "areaAtuacaoMedico": "",
            },
            "page": page,
            "pageNumber": page,
            "pageSize": page_size,
        }
    ]


async def _get_captcha_token(db_pool: asyncpg.Pool) -> str:
    """ObtÃ©m o token do captcha do banco.

    Raises:
        RuntimeError: Se nÃ£o houver token vÃ¡lido.
    """
    token = await captcha_db.get_token(db_pool)
    if not token:
        raise RuntimeError(
            "âŒ Token do captcha nÃ£o encontrado ou expirado!\n"
            "   Execute primeiro: uv run cfm token"
        )

    ttl = await captcha_db.get_ttl(db_pool)
    print(f"âœ… Token do captcha obtido (TTL restante: {ttl}s)")
    return token


async def _validate_captcha_token(db_pool: asyncpg.Pool) -> bool:
    """Verifica se o token do captcha ainda Ã© vÃ¡lido."""
    return await captcha_db.is_valid(db_pool)


async def _get_captcha_ttl(db_pool: asyncpg.Pool) -> int:
    """Retorna o TTL restante do token do captcha em segundos."""
    return await captcha_db.get_ttl(db_pool)


async def _intercept_search_response(page: Page) -> dict:
    """Intercepta a resposta da API buscar_medicos disparada pelo formulÃ¡rio."""
    future: asyncio.Future[dict] = asyncio.get_event_loop().create_future()

    async def _on_response(response: Response) -> None:
        if "buscar_medicos" in response.url:
            try:
                data = await response.json()
                if not future.done():
                    future.set_result(data)
            except Exception:
                pass

    page.on("response", _on_response)

    result = await future
    page.remove_listener("response", _on_response)
    return result


async def _select_uf_and_search(page: Page, uf: str) -> None:
    """Seleciona a UF no formulÃ¡rio de busca."""
    await page.select_option('select[name="uf"]', uf)
    await page.wait_for_timeout(500)


async def fetch_medicos_page(
    page: Page,
    captcha_token: str,
    uf: str,
    municipio: str = "",
    current_page: int = 1,
    page_size: int = 100,
    request_timeout: int = 120,
) -> tuple[list[MedicoRaw], int]:
    """Busca uma pÃ¡gina de mÃ©dicos via fetch no contexto do browser.

    Returns:
        Tupla com (lista de MedicoRaw, total de registros).
    """
    payload = _build_search_payload(
        captcha_token=captcha_token,
        uf=uf,
        municipio=municipio,
        page=current_page,
        page_size=page_size,
    )

    js_timeout_ms = (request_timeout - 5) * 1000

    try:
        data = await asyncio.wait_for(
            page.evaluate(
                """async ([url, payload, timeoutMs]) => {
                    const controller = new AbortController();
                    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

                    try {
                        const resp = await fetch(url, {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify(payload),
                            signal: controller.signal
                        });
                        clearTimeout(timeoutId);
                        return await resp.json();
                    } catch (error) {
                        clearTimeout(timeoutId);
                        if (error.name === 'AbortError') {
                            throw new Error(`Request timeout (${timeoutMs}ms)`);
                        }
                        throw error;
                    }
                }""",
                [CFM_BUSCA_URL, payload, js_timeout_ms],
            ),
            timeout=request_timeout,
        )
    except asyncio.TimeoutError:
        raise Exception(
            f"Timeout de {request_timeout}s ao buscar pÃ¡gina {current_page} da UF {uf}"
        )

    if data.get("status") != "sucesso":
        raise Exception(f"API retornou erro: {data}")

    dados = data.get("dados", [])
    if not dados:
        return [], 0

    total_count = int(dados[0].get("COUNT", 0))

    adapter = TypeAdapter(list[MedicoRaw])
    medicos = adapter.validate_python(dados)

    return medicos, total_count


async def fetch_foto_medico(
    page: Page,
    crm: str,
    uf: str,
    security_hash: str,
) -> MedicoFotoRaw | None:
    """Busca os detalhes/foto de um mÃ©dico via POST no contexto do browser."""
    try:
        payload = [{"securityHash": security_hash, "crm": crm, "uf": uf}]
        data = await page.evaluate(
            """async ([url, payload, timeoutMs]) => {
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

                try {
                    const resp = await fetch(url, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(payload),
                        signal: controller.signal
                    });
                    clearTimeout(timeoutId);
                    const text = await resp.text();
                    try {
                        return JSON.parse(text);
                    } catch {
                        return { status: 'erro', mensagem: text };
                    }
                } catch (error) {
                    clearTimeout(timeoutId);
                    if (error.name === 'AbortError') {
                        return { status: 'erro', mensagem: `Timeout apÃ³s ${timeoutMs}ms` };
                    }
                    throw error;
                }
            }""",
            [CFM_FOTO_URL, payload, 30000],
        )

        if data.get("status") == "sucesso" and data.get("dados"):
            return MedicoFotoRaw(**data["dados"][0])
    except Exception as e:
        print(f"âš ï¸ Erro ao buscar foto CRM {crm}/{uf}: {e}")

    return None


def _format_doctor_for_db(
    raw: MedicoRaw, raw_data: dict, foto: MedicoFotoRaw | None = None
) -> dict:
    """Formata um mÃ©dico para persistÃªncia no banco.

    Aplica title_case_br, parseia especialidades, traduz chaves para EN
    e inclui raw_data.
    """
    medico = Medico.from_raw(raw, foto=foto)
    medico_dict = medico.model_dump(mode="json")

    # Parseia especialidades de string para lista de dicts
    specialties_json = parse_specialties(medico_dict.get("especialidade"))

    # Traduz chaves para EN
    doc = translate_keys_to_en(medico_dict)

    # Formata campos com title_case_br
    doc["name"] = title_case_br(doc.get("name"))
    doc["social_name"] = title_case_br(doc.get("social_name"))
    doc["graduation_institution"] = title_case_br(doc.get("graduation_institution"))

    # Formata nomes das especialidades
    for spec in specialties_json:
        spec["name"] = title_case_br(spec.get("name"))

    doc["specialties"] = specialties_json
    doc["raw_data"] = raw_data

    return doc


async def _fetch_batch(
    page: Page,
    captcha_token: str,
    uf: str,
    pages: list[int],
    page_size: int,
    request_timeout: int = 120,
) -> list[tuple[int, list[MedicoRaw], int]]:
    """Busca vÃ¡rias pÃ¡ginas em paralelo.

    Returns:
        Lista de tuplas (page_number, medicos, total_count).
    """
    tasks = [
        fetch_medicos_page(
            page=page,
            captcha_token=captcha_token,
            uf=uf,
            current_page=p,
            page_size=page_size,
            request_timeout=request_timeout,
        )
        for p in pages
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    successful: list[tuple[int, list[MedicoRaw], int]] = []
    failed_pages: list[int] = []

    for p, result in zip(pages, results):
        if isinstance(result, Exception):
            print(f"âš ï¸ Erro na pÃ¡gina {p}: {result}")
            failed_pages.append(p)
        else:
            medicos, total_count = result
            successful.append((p, medicos, total_count))

    # Retry das pÃ¡ginas que falharam (atÃ© 2 tentativas individuais)
    for retry_attempt in range(1, 3):
        if not failed_pages:
            break

        print(
            f"ðŸ”„ Retry {retry_attempt}/2 de {len(failed_pages)} pÃ¡gina(s): {failed_pages}"
        )
        await asyncio.sleep(2)

        still_failed: list[int] = []
        for p in failed_pages:
            try:
                medicos, total_count = await fetch_medicos_page(
                    page=page,
                    captcha_token=captcha_token,
                    uf=uf,
                    current_page=p,
                    page_size=page_size,
                    request_timeout=request_timeout,
                )
                successful.append((p, medicos, total_count))
                print(f"   âœ… PÃ¡gina {p} recuperada no retry {retry_attempt}")
            except Exception as e:
                print(f"   âš ï¸ PÃ¡gina {p} falhou novamente: {e}")
                still_failed.append(p)

        failed_pages = still_failed

    if failed_pages:
        print(f"âŒ PÃ¡ginas nÃ£o recuperadas apÃ³s retries: {failed_pages}")

    return successful


async def crawl_state(
    page: Page,
    uf: str,
    execution_state_id: int,
    db_pool: asyncpg.Pool,
    page_size: int = 1000,
    delay: float = 0.8,
    fetch_fotos: bool = True,
    max_results: int = 0,
    request_timeout: int = 120,
    batch_size: int = 5,
) -> int:
    """Crawla todos os mÃ©dicos de uma UF usando o plano de execuÃ§Ã£o.

    Usa a tabela crawl_pages para saber exatamente quais pÃ¡ginas faltam.
    Ao descobrir total_pages, prÃ©-cria todas como 'pending'.
    Retoma de onde parou buscando apenas pÃ¡ginas pending/failed.

    Returns:
        Total de mÃ©dicos processados para esta UF nesta sessÃ£o.
    """
    print(f"\n{'=' * 60}")
    print(f"ðŸ¥ Iniciando crawl da UF: {uf}")
    print(f"âš¡ Batch size: {batch_size} | Timeout: {request_timeout}s")
    print(f"{'=' * 60}")

    await start_execution_state(db_pool, execution_state_id)

    captcha_token = await _get_captcha_token(db_pool)

    total_medicos = 0
    total_count = 0
    total_pages = None
    retries = 0
    max_retries = 3
    consecutive_empty_batches = 0
    max_empty_batches = 2  # Pausa apÃ³s N batches consecutivos com 0 mÃ©dicos

    batch_times: list[float] = []
    total_start_time = time.time()

    # Carregar total_pages/total_records do banco (para retomada)
    state_row = await db_pool.fetchrow(
        "SELECT total_pages, total_records FROM crawl_execution_states WHERE id = $1",
        execution_state_id,
    )
    if state_row and state_row["total_pages"]:
        total_pages = state_row["total_pages"]
        total_count = state_row["total_records"] or 0

    # Busca a primeira pÃ¡gina para descobrir total_count e total_pages
    pending = await get_pending_pages(db_pool, execution_state_id, limit=1)

    # Se nÃ£o hÃ¡ pÃ¡ginas pendentes no banco, precisamos fazer a descoberta inicial
    first_page = pending[0] if pending else 1
    discovery_needed = not pending  # Se nÃ£o hÃ¡ nenhuma pÃ¡gina, precisa descobrir

    if not discovery_needed:
        # Verifica se jÃ¡ inicializou (tem pÃ¡ginas no banco)
        all_pending = await get_pending_pages(db_pool, execution_state_id)
        if all_pending:
            print(f"ðŸ”„ Retomando: {len(all_pending)} pÃ¡ginas pendentes")
        else:
            # Todas fetched â€” verificar completude
            if await check_state_complete(db_pool, execution_state_id):
                print(f"âœ… UF {uf} jÃ¡ estÃ¡ completa.")
                return 0

    while True:
        if not await _validate_captcha_token(db_pool):
            print("âŒ Token do captcha expirou! Execute: uv run cfm token")
            await fail_execution_state(db_pool, execution_state_id)
            raise RuntimeError("Token do captcha expirado durante o crawl.")

        # Busca pÃ¡ginas pendentes do banco
        pending_pages = await get_pending_pages(
            db_pool, execution_state_id, limit=batch_size
        )

        if not pending_pages:
            # Sem mais pÃ¡ginas pendentes
            if total_pages is None:
                # Primeira execuÃ§Ã£o â€” busca pÃ¡gina 1 para descobrir total
                pending_pages = [1]
            else:
                break

        batch_start = time.time()

        try:
            results = await _fetch_batch(
                page=page,
                captcha_token=captcha_token,
                uf=uf,
                pages=pending_pages,
                page_size=page_size,
                request_timeout=request_timeout,
            )
            batch_time = time.time() - batch_start
            batch_times.append(batch_time)
            retries = 0
        except Exception as e:
            retries += 1
            print(f"âš ï¸ Erro no batch (tentativa {retries}/{max_retries}): {e}")

            if retries >= max_retries:
                if await _validate_captcha_token(db_pool):
                    captcha_token = await _get_captcha_token(db_pool)
                    retries = 0
                    continue
                else:
                    print("âŒ Token expirado e mÃ¡ximo de retries atingido.")
                    await fail_execution_state(db_pool, execution_state_id)
                    raise RuntimeError(
                        f"Falha apÃ³s {max_retries} tentativas para UF {uf}. "
                        "Execute: uv run cfm token"
                    )

            await asyncio.sleep(delay)
            continue

        if not results:
            retries += 1
            if retries >= max_retries:
                if await _validate_captcha_token(db_pool):
                    captcha_token = await _get_captcha_token(db_pool)
                    retries = 0
                    continue
                else:
                    await fail_execution_state(db_pool, execution_state_id)
                    raise RuntimeError(
                        f"Todas as {len(pending_pages)} requests falharam para UF {uf}."
                    )
            await asyncio.sleep(delay)
            continue

        results.sort(key=lambda r: r[0])

        batch_medicos: list[dict] = []
        fetched_page_records: list[tuple[int, int]] = []  # (page_number, count)
        blocked_page_numbers: list[
            int
        ] = []  # pÃ¡ginas com 0 mÃ©dicos (servidor bloqueou)
        failed_page_numbers: list[int] = set(pending_pages) - {r[0] for r in results}

        for page_num, raw_medicos, page_total_count in results:
            if page_total_count > 0:
                total_count = page_total_count
                new_total_pages = math.ceil(total_count / page_size)

                if total_pages is None or new_total_pages != total_pages:
                    total_pages = new_total_pages
                    # PrÃ©-criar/atualizar pÃ¡ginas no banco
                    inserted = await initialize_pages(
                        db_pool, execution_state_id, total_pages
                    )
                    await update_execution_state(
                        db_pool,
                        execution_state_id,
                        total_pages=total_pages,
                        total_records=total_count,
                    )
                    if inserted > 0:
                        print(
                            f"ðŸ“‹ Total: {total_count} mÃ©dicos em {total_pages} pÃ¡ginas "
                            f"({inserted} novas pÃ¡ginas criadas)"
                        )

            record_count = len(raw_medicos) if raw_medicos else 0

            # Detectar bloqueio do servidor: pÃ¡gina retorna 0 mÃ©dicos
            # mas sabemos que deveria ter (total_count jÃ¡ conhecido)
            if record_count == 0 and total_count > 0:
                blocked_page_numbers.append(page_num)
            else:
                fetched_page_records.append((page_num, record_count))

            if raw_medicos:
                for raw in raw_medicos:
                    raw_data = raw.model_dump(mode="json", by_alias=True)
                    doc = _format_doctor_for_db(raw, raw_data=raw_data, foto=None)
                    batch_medicos.append(doc)

        # Marcar pÃ¡ginas no banco
        if fetched_page_records:
            await mark_pages_fetched_batch(
                db_pool, execution_state_id, fetched_page_records
            )
        for fp in failed_page_numbers:
            await mark_page_failed(
                db_pool, execution_state_id, fp, "Sem resultado no batch"
            )
        for bp in blocked_page_numbers:
            await mark_page_failed(
                db_pool,
                execution_state_id,
                bp,
                "Servidor retornou 0 mÃ©dicos (possÃ­vel bloqueio)",
            )

        # Detectar bloqueio consecutivo do servidor
        if batch_medicos:
            consecutive_empty_batches = 0
        elif total_count > 0:
            # Batch inteiro retornou 0 mÃ©dicos â€” possÃ­vel bloqueio
            consecutive_empty_batches += 1
            if consecutive_empty_batches >= max_empty_batches:
                print(
                    f"\nðŸš« Servidor bloqueou! {consecutive_empty_batches} batches "
                    f"consecutivos retornaram 0 mÃ©dicos."
                )
                print(
                    f"   {len(blocked_page_numbers)} pÃ¡ginas marcadas como falha para retry."
                )
                print(f"   Pausando execuÃ§Ã£o. Resolva novo captcha e retome:")
                print(f"   uv run cfm token && uv run cfm run <ID>")
                await fail_execution_state(db_pool, execution_state_id)
                raise RuntimeError(
                    "Servidor bloqueou a requisiÃ§Ã£o (0 mÃ©dicos retornados). "
                    "Resolva novo captcha: uv run cfm token"
                )

        # Calcular progresso
        remaining_pages = await get_pending_pages(db_pool, execution_state_id)
        remaining_count = len(remaining_pages)

        fetched_count = (total_pages or 0) - remaining_count if total_pages else 0
        percentage = round(fetched_count / total_pages * 100, 1) if total_pages else 0

        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0
        if remaining_count > 0:
            batches_remaining = math.ceil(remaining_count / batch_size)
            eta_seconds = batches_remaining * (avg_batch_time + delay)
            eta_minutes = int(eta_seconds / 60)
        else:
            eta_minutes = 0

        eta_info = ""
        if eta_minutes > 60:
            eta_hours = eta_minutes // 60
            eta_mins = eta_minutes % 60
            eta_info = f" | ETA: ~{eta_hours}h{eta_mins}m"
        elif eta_minutes > 0:
            eta_info = f" | ETA: ~{eta_minutes}m"

        ttl_info = ""
        if fetched_count <= batch_size or fetched_count % 20 == 0:
            ttl = await _get_captcha_ttl(db_pool)
            if ttl > 0:
                ttl_info = f" | TTL: {ttl}s"
                if ttl < 120:
                    ttl_info += " âš ï¸"

        page_range = (
            f"{min(pending_pages)}-{max(pending_pages)}" if pending_pages else "?"
        )
        successful = len(results)
        fail_count = len(failed_page_numbers)
        fail_info = f" ({fail_count} falhas)" if fail_count > 0 else ""

        print(
            f"ðŸ“¡ PÃ¡ginas {page_range}"
            f"/{total_pages or '?'}: {len(batch_medicos)} mÃ©dicos "
            f"({percentage}%) | "
            f"{batch_time:.2f}s ({successful}pg){fail_info}{eta_info}{ttl_info}"
        )

        if batch_medicos:
            process_start = time.time()
            await upsert_doctors_batch(db_pool, batch_medicos)
            process_time = time.time() - process_start

            if process_time > 1.0:
                print(f"   ðŸ’¾ Insert: {process_time:.2f}s")

            total_medicos += len(batch_medicos)

        if max_results > 0 and total_medicos >= max_results:
            print(f"ðŸ›‘ Limite de teste atingido: {total_medicos}/{max_results}")
            break

        # Verificar se terminou
        if remaining_count == 0:
            break

        await asyncio.sleep(delay)

    # Verificar completude do estado
    is_complete = await check_state_complete(db_pool, execution_state_id)

    total_time = time.time() - total_start_time
    total_minutes = int(total_time / 60)
    total_seconds = int(total_time % 60)

    status_icon = "âœ…" if is_complete else "â¸ï¸"
    print(f"\n{'=' * 60}")
    print(
        f"{status_icon} {total_medicos} mÃ©dicos processados nesta sessÃ£o para UF {uf}."
    )
    print(f"â±ï¸  Tempo total: {total_minutes}m {total_seconds}s")
    if batch_times:
        avg_time = sum(batch_times) / len(batch_times)
        print(f"âš¡ Tempo mÃ©dio por batch ({batch_size}pg): {avg_time:.2f}s")
    print(f"{'=' * 60}")

    return total_medicos


async def run_execution(
    page: Page,
    execution_id: int,
    db_pool: asyncpg.Pool,
    page_size: int = 1000,
    batch_size: int = 5,
    delay: float = 0.8,
    fetch_fotos: bool = True,
    max_results: int = 0,
    request_timeout: int = 120,
) -> int:
    """Executa um plano de execuÃ§Ã£o completo.

    Itera sobre os estados pendentes da execuÃ§Ã£o, chamando crawl_state para cada.
    Trata interrupÃ§Ãµes e atualiza status.

    Returns:
        Total de mÃ©dicos processados na sessÃ£o.
    """
    await start_execution(db_pool, execution_id)
    total_medicos = 0

    try:
        pending_states = await get_pending_states(db_pool, execution_id)

        if not pending_states:
            print("â„¹ï¸  Todos os estados jÃ¡ foram processados.")
            await check_execution_complete(db_pool, execution_id)
            return 0

        print(f"ðŸ“‹ Estados pendentes: {', '.join(s['state'] for s in pending_states)}")

        for state_record in pending_states:
            uf = state_record["state"]
            state_id = state_record["id"]

            try:
                count = await crawl_state(
                    page=page,
                    uf=uf,
                    execution_state_id=state_id,
                    db_pool=db_pool,
                    page_size=page_size,
                    delay=delay,
                    fetch_fotos=fetch_fotos,
                    max_results=max_results,
                    request_timeout=request_timeout,
                    batch_size=batch_size,
                )
                total_medicos += count

            except RuntimeError as e:
                if "captcha" in str(e).lower():
                    print(f"\nâŒ Token do captcha expirou durante o crawl de {uf}.")
                    print("   Execute: uv run cfm token")
                    await pause_execution(db_pool, execution_id)
                    raise
                print(f"âŒ Erro ao processar UF {uf}: {e}")
                continue
            except Exception as e:
                print(f"âŒ Erro ao processar UF {uf}: {e}")
                await fail_execution_state(db_pool, state_id)
                continue

        # Verificar se completou tudo
        is_complete = await check_execution_complete(db_pool, execution_id)
        if is_complete:
            print(f"\nðŸŽ‰ ExecuÃ§Ã£o #{execution_id} concluÃ­da com sucesso!")
        else:
            print(
                f"\nâ¸ï¸ ExecuÃ§Ã£o #{execution_id} pausada. Use 'cfm run {execution_id}' para continuar."
            )
            await pause_execution(db_pool, execution_id)

    except KeyboardInterrupt:
        print(f"\n\nðŸ›‘ Interrompido pelo usuÃ¡rio. ExecuÃ§Ã£o #{execution_id} pausada.")
        await pause_execution(db_pool, execution_id)
        raise

    return total_medicos


def save_medicos_to_json(medicos: list[Medico], output_dir: str, uf: str) -> Path:
    """Salva mÃ©dicos em arquivo JSON (legacy/backup).

    .. deprecated::
        Use a persistÃªncia no PostgreSQL via crawl_uf().
    """
    output_path = Path(output_dir) / "cfm"
    output_path.mkdir(parents=True, exist_ok=True)

    file_path = output_path / f"medicos_{uf.lower()}.json"

    data = [medico.model_dump(mode="json") for medico in medicos]

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ðŸ’¾ {len(medicos)} mÃ©dicos salvos em {file_path}")
    return file_path
